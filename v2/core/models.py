"""
Pydantic models for MoceanAI V2.

This module defines all data structures used throughout the pipeline:

  1. **LLM structured output models** — Pydantic models passed to
     `model.with_structured_output()` so the LLM returns validated JSON.
     Each has exactly the fields the corresponding prompt expects.

  2. **Word-level alignment** — Represents timing data from ElevenLabs'
     character-to-word mapping, used for precise audio-visual sync.

  3. **Timing / visual planning** — Intermediate data structures that
     track how long each segment lasts and how many images it needs.

  4. **Pipeline state models** — ShortFormState and LongFormState hold
     every piece of data flowing through the pipeline. They start with
     user inputs and accumulate generated content step by step. When a
     step fails, the state already contains everything from previous
     steps, so no paid API work is lost.

To extend:
  - Adding a new LLM output model: create a BaseModel subclass here,
    then use it in script_generator.py with `generate_structured()`.
  - Adding a field to the pipeline state: add it to ShortFormState or
    LongFormState with a default value, then populate it in pipeline_runner.py.
"""

from __future__ import annotations

from pathlib import Path
from typing import Optional

from pydantic import BaseModel, Field


# ═══════════════════════════════════════════════════════════════════════════
# LLM STRUCTURED OUTPUT MODELS
# These are used with langchain's .with_structured_output() to ensure
# the LLM returns valid, typed JSON matching the schema below.
# ═══════════════════════════════════════════════════════════════════════════

class GoalContainer(BaseModel):
    """LLM output: the strategic goal / CTA for the video."""
    goal: str = Field(..., description="The goal of the video.")


class HookContainer(BaseModel):
    """LLM output: the opening hook that stops the scroll."""
    hook: str = Field(..., description="The opening hook for the video.")


class ScriptContainer(BaseModel):
    """LLM output: the full spoken narration script."""
    script: str = Field(..., description="The full narration script.")


class EnhancedScriptContainer(BaseModel):
    """LLM output: script with ElevenLabs audio tags ([whispers], [pause], etc.)."""
    enhanced_script: str = Field(..., description="Script enhanced with ElevenLabs audio tags.")


class ScriptSegment(BaseModel):
    """
    A single beat of the script with both raw and enhanced versions.
    The raw version is used for image description generation.
    The enhanced version is sent to ElevenLabs for TTS.
    """
    script_segment: str = Field(..., description="A segment from the script.")
    enhanced_script_segment: str = Field(..., description="Enhanced version of the segment.")


class ScriptListContainer(BaseModel):
    """LLM output: the full script split into aligned segment pairs."""
    script_list: list[ScriptSegment]


class ImageDescription(BaseModel):
    """
    A detailed text prompt for generating one B-roll image.
    The description is exhaustively detailed for AI image generators.
    uses_logo is True only when the segment references branding/CTAs.
    """
    description: str = Field(..., description="Detailed image prompt for generation.")
    uses_logo: bool = Field(False, description="Whether the image should include a logo.")


class SegmentImageDescriptionsContainer(BaseModel):
    """LLM output: image descriptions for all images in one segment."""
    segment_image_descriptions: list[ImageDescription]


class TopicsContainer(BaseModel):
    """LLM output: extracted video topics from messy user input."""
    topics: list[str] = Field(..., description="Extracted video topics.")


# --- Long-form specific models ---

class SectionStructure(BaseModel):
    """
    Blueprint for one section of a long-form video.
    Generated by the structure-planning LLM call.
    Contains enough detail for a scriptwriter to develop 1-3 minutes of content.
    """
    section_name: str = Field(..., description="Descriptive name for the section.")
    section_purpose: str = Field(..., description="Strategic function of this section.")
    section_directives: list[str] = Field(..., description="Meta-instructions for execution.")
    section_talking_points: list[str] = Field(..., description="Content elements to cover.")


class SectionsStructureContainer(BaseModel):
    """LLM output: the full video structure (5-8 sections)."""
    sections_structure_list: list[SectionStructure]


class SectionScriptContainer(BaseModel):
    """LLM output: narration script for one section."""
    section_script: str = Field(..., description="Complete narration for one section.")


class SectionScriptSegmentItem(BaseModel):
    """One vocal-unit segment within a section's script."""
    script_segment: str = Field(..., description="A segment of a section script.")


class SectionScriptSegmentedContainer(BaseModel):
    """LLM output: a section script split into vocal-unit segments."""
    script_segment_list: list[SectionScriptSegmentItem]


# ═══════════════════════════════════════════════════════════════════════════
# WORD-LEVEL ALIGNMENT
# Represents timing data extracted from ElevenLabs' character-level
# alignment. We group characters into words so we can map script
# segments to precise audio timestamps without the fragile character-
# index offset hack from v1.
# ═══════════════════════════════════════════════════════════════════════════

class WordAlignment(BaseModel):
    """One word from the TTS output with its start and end time in seconds."""
    word: str
    start_time: float
    end_time: float


# ═══════════════════════════════════════════════════════════════════════════
# TIMING AND VISUAL PLANNING
# These models bridge the audio timeline to the visual pipeline.
# ═══════════════════════════════════════════════════════════════════════════

class SegmentTiming(BaseModel):
    """When a script segment starts and ends in the generated audio."""
    start_time: float
    end_time: float
    duration: float


class SegmentVisualPlan(BaseModel):
    """
    Visual production plan for one script segment.

    Tracks the full lifecycle of a segment's visuals:
      1. Planning:     num_images and last_image_duration (from timing math)
      2. Descriptions: image_descriptions (from LLM)
      3. Images:       image_paths (from image provider API)
      4. Video:        video_path (from zoompan animation or video gen API)
    """
    segment_index: int                                          # Position in the segment list
    num_images: int                                             # How many B-roll images this segment needs
    last_image_duration: float                                  # Duration of the final image (may differ from ideal)
    image_descriptions: list[ImageDescription] = Field(default_factory=list)  # AI-generated image prompts
    image_paths: list[Path] = Field(default_factory=list)       # Paths to generated image files on disk
    video_path: Optional[Path] = None                           # Path to the animated clip (zoompan or video gen)


# ═══════════════════════════════════════════════════════════════════════════
# PIPELINE STATE — SHORT-FORM
#
# This model accumulates ALL data as it flows through the pipeline.
# Fields start as None and are populated step by step. If the pipeline
# fails at step 8, fields from steps 1-7 are preserved in the state,
# which is attached to the PipelineError for the UI to display.
# ═══════════════════════════════════════════════════════════════════════════

class ShortFormState(BaseModel):
    """Tracks all data through the short-form video pipeline."""

    # --- User inputs (set before pipeline starts) ---
    topic: str
    purpose: str
    target_audience: str
    tone: str
    platform: str
    duration_seconds: int
    orientation: str = "Portrait"
    model_provider: str = "google"          # LLM provider for script generation
    image_provider: str = "google"          # Image gen provider (Google/OpenAI/Flux)
    image_style: str = "Isometric Illustrations"
    voice_actor: str = "american_female_conversationalist"
    voice_model_version: str = "eleven_v3"
    additional_instructions: Optional[str] = None
    additional_image_requests: Optional[str] = None
    style_reference: str = ""
    enhance_for_tts: bool = True            # Whether to add ElevenLabs audio tags
    add_subtitles: bool = False             # Whether to burn SRT subtitles into video
    add_end_buffer: bool = True             # Append black frame at the end
    # Visual mode: "zoompan" (image + camera motion) or "video_gen" (AI video API)
    visual_mode: str = "zoompan"
    # Which video generation provider to use when visual_mode == "video_gen"
    video_provider: str = "runway"
    # Whether AI-generated images may include visible human faces
    allow_faces: bool = False

    # --- Generated content (populated step by step) ---
    goal: Optional[str] = None
    hook: Optional[str] = None
    script: Optional[str] = None
    enhanced_script: Optional[str] = None

    # --- Audio data ---
    audio_path: Optional[Path] = None                   # Path to the generated .mp3
    word_alignments: list[WordAlignment] = Field(default_factory=list)  # Per-word timestamps

    # --- Segment data ---
    segments: list[ScriptSegment] = Field(default_factory=list)         # Script split into clips
    segment_timings: list[SegmentTiming] = Field(default_factory=list)  # Audio timing per segment
    segment_visual_plans: list[SegmentVisualPlan] = Field(default_factory=list)  # Images/clips per segment

    # --- Final output ---
    clip_paths: list[Path] = Field(default_factory=list)   # All animated clip paths
    final_video_path: Optional[Path] = None                # The assembled final video

    # --- Tuning parameters ---
    ideal_image_duration: float = 3.0       # Target seconds per image
    min_image_duration: float = 2.0         # Minimum standalone image duration
    # When True, each segment gets exactly one image for its full duration
    # instead of dividing into multiple images every ~3 seconds
    single_image_per_segment: bool = False


# ═══════════════════════════════════════════════════════════════════════════
# PIPELINE STATE — LONG-FORM (section-based)
# ═══════════════════════════════════════════════════════════════════════════

class SectionState(BaseModel):
    """
    Tracks all data for a single section in the long-form pipeline.
    Each section independently has its own script, audio, images, and video.
    """
    section_structure: SectionStructure                         # Blueprint from structure generation
    section_script: Optional[str] = None                       # Written narration for this section
    audio_path: Optional[Path] = None                          # Section's audio file
    word_alignments: list[WordAlignment] = Field(default_factory=list)
    segments: list[SectionScriptSegmentItem] = Field(default_factory=list)  # Script segments
    segment_timings: list[SegmentTiming] = Field(default_factory=list)
    segment_visual_plans: list[SegmentVisualPlan] = Field(default_factory=list)
    clip_paths: list[Path] = Field(default_factory=list)       # Animated clips
    section_video_path: Optional[Path] = None                  # Assembled section video


class LongFormState(BaseModel):
    """Tracks all data through the long-form video pipeline."""

    # --- User inputs ---
    topic: str
    purpose: str
    target_audience: str
    tone: str
    platform: str
    duration_seconds: int = 600
    orientation: str = "Landscape"
    model_provider: str = "google"
    image_provider: str = "google"
    image_style: str = "Cinematic"
    voice_actor: str = "american_female_conversationalist"
    voice_model_version: str = "eleven_v3"
    additional_instructions: Optional[str] = None
    additional_image_requests: Optional[str] = None
    style_reference: str = ""
    enhance_for_tts: bool = False
    add_subtitles: bool = False
    visual_mode: str = "zoompan"
    video_provider: str = "runway"
    # Whether AI-generated images may include visible human faces
    allow_faces: bool = False

    # --- Generated content ---
    goal: Optional[str] = None
    sections: list[SectionState] = Field(default_factory=list)  # One per video section
    full_script: Optional[str] = None                           # All sections concatenated

    # --- Final output ---
    final_video_path: Optional[Path] = None

    # --- Tuning parameters ---
    ideal_image_duration: float = 3.0
    min_image_duration: float = 2.0
    # When True, each segment gets exactly one image for its full duration
    single_image_per_segment: bool = False
